# -*- coding: utf-8 -*-
"""NLP_TextualRelatedness_STR

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZpPhvekU4761ffnv_eKEFQPqwjbd6ky4
"""

import pandas as pd
from google.colab import drive
import os

drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/JMU_Wu/NLP/eng_train.csv'
df = pd.read_csv(file_path)


def split_sentences(sentence):
    sentences = sentence.split('\n')
    first_sentence = sentences[0].strip()
    second_sentence = sentences[1].strip() if len(sentences) > 1 else ''
    return first_sentence, second_sentence

pair_id = df['PairID']
text = df['Text']
score = df['Score']

sentences = [split_sentences(sentence) for sentence in text]

supervised_data = pd.DataFrame({
    'PairID': pair_id,
    'FirstSentence': [sentence[0] for sentence in sentences],
    'SecondSentence': [sentence[1] for sentence in sentences],
    'Score': score
})

save_path = '/content/drive/MyDrive/JMU_Wu/NLP/'
supervised_data.to_csv(os.path.join(save_path, 'supervised_data.csv'), index=False)

import pandas as pd
import os


supervised_data = pd.read_csv('/content/drive/MyDrive/JMU_Wu/NLP/supervised_data.csv')


cleaned_data = supervised_data.dropna(subset=['SecondSentence'])


save_path = '/content/drive/MyDrive/JMU_Wu/NLP/'
cleaned_data.to_csv(os.path.join(save_path, 'cleaned_data.csv'), index=False)

pip install python-Levenshtein

import pandas as pd
import re

cleaned_data = pd.read_csv('/content/drive/MyDrive/JMU_Wu/NLP/cleaned_data.csv')

def remove_punctuation(text):
    pattern = '[^\w\s]'
    text = re.sub(pattern, '', text)
    return text

cleaned_data['CleanedFirstSentence'] = cleaned_data['FirstSentence'].apply(remove_punctuation)
cleaned_data['CleanedSecondSentence'] = cleaned_data['SecondSentence'].apply(remove_punctuation)

cleaned_data.to_csv('/content/drive/MyDrive/JMU_Wu/NLP/cleaned_data_nopunc.csv', index=False)

pip install nltk

import nltk
nltk.download('punkt')

pip install --upgrade torch torchvision torchaudio

pip install --upgrade transformers

import os
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
import pandas as pd
import time
import warnings
import sys

# Suppress warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


from transformers import logging
logging.set_verbosity_error()

df = pd.read_csv('/content/drive/MyDrive/JMU_Wu/NLP/cleaned_data_nopunc.csv')

# Split - train and validation
train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)

class SemanticRelatednessDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pair = self.data.iloc[idx]
        first_sentence = pair['FirstSentence']
        second_sentence = pair['SecondSentence']
        relatedness_score = pair['Score']

        inputs = self.tokenizer(
            first_sentence,
            second_sentence,
            return_tensors='pt',
            max_length=self.max_length,
            truncation=True,
            padding='max_length'
        )

        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': torch.tensor(relatedness_score, dtype=torch.float32)
        }

# Create the RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)

train_dataset = SemanticRelatednessDataset(train_df, tokenizer)
val_dataset = SemanticRelatednessDataset(val_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 1

start_time = time.time()

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    average_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}/{num_epochs} - Average Loss: {average_loss}")

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Total Training Time: {elapsed_time} seconds")

model_save_path = "semantic_relatedness_model"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

#Visual Inspection
import time

start_time = time.time()
end_time = 0
elapsed_time = 0
model.eval()
with torch.no_grad():
    for i, batch in enumerate(val_loader):
        # if i >= 5:
        #     break

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        predicted_scores = outputs.logits.squeeze().cpu().numpy()

        print(f"Case {i + 1}:")
        print("Predicted Score:", predicted_scores)
        print("Actual Score:", labels.cpu().numpy())
        print()

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Total Time taken: {elapsed_time} seconds")

pip install scipy

from scipy.stats import pearsonr
import time

start_time = time.time()
end_time = 0
elapsed_time = 0
model.eval()
predicted_scores_all = []
labels_all = []

with torch.no_grad():
    for i, batch in enumerate(val_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        predicted_scores = outputs.logits.squeeze().cpu().numpy()

        # Accumulate predicted scores and labels
        predicted_scores_all.extend(predicted_scores)
        labels_all.extend(labels.cpu().numpy())

# Calculate Pearson correlation coefficient
correlation_coefficient, _ = pearsonr(predicted_scores_all, labels_all)
print("Pearson Correlation Coefficient:", correlation_coefficient)

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Total Time taken: {elapsed_time} seconds")

from scipy.stats import spearmanr
import time

start_time = time.time()
end_time = 0
elapsed_time = 0
model.eval()
predicted_scores_all = []
labels_all = []

with torch.no_grad():
    for i, batch in enumerate(val_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        predicted_scores = outputs.logits.squeeze().cpu().numpy()

        # Accumulate predicted scores and labels
        predicted_scores_all.extend(predicted_scores)
        labels_all.extend(labels.cpu().numpy())

# Calculate Spearman correlation coefficient
correlation_coefficient, _ = spearmanr(predicted_scores_all, labels_all)
print("Spearman Correlation Coefficient:", correlation_coefficient)

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Total Time taken: {elapsed_time} seconds")

import torch
from torch.utils.data import DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
import pandas as pd
import time
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, spearmanr

import time

start_time = time.time()
end_time = 0
elapsed_time = 0

df = pd.read_csv('/content/drive/MyDrive/JMU_Wu/NLP/cleaned_data_nopunc.csv')

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

def create_dataset(dataframe, tokenizer, max_length=128):
    return SemanticRelatednessDataset(dataframe, tokenizer, max_length)

learning_rates = [5e-5, 3e-5, 2e-5]
epochs_to_run = [3, 5]
seeds = [42, 123, 456]

all_final_accuracies = []
all_epoch_losses = []
all_pearson_correlations = []
all_spearman_correlations = []

for learning_rate in learning_rates:
    for num_epochs in epochs_to_run:
        for seed in seeds:
            tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
            model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)

            train_dataset = create_dataset(train_df, tokenizer)
            val_dataset = create_dataset(val_df, tokenizer)

            train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
            optimizer = AdamW(model.parameters(), lr=learning_rate)
            num_epochs = num_epochs

            epoch_losses = []
            epoch_accuracies = []

            start_time = time.time()

            for epoch in range(num_epochs):
                model.train()
                total_loss = 0

                for batch in train_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)

                    optimizer.zero_grad()

                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                    total_loss += loss.item()

                    loss.backward()
                    optimizer.step()

                average_loss = total_loss / len(train_loader)
                epoch_losses.append(average_loss)

                model.eval()
                total_correct = 0
                total_samples = 0

                with torch.no_grad():
                    for batch in val_loader:
                        input_ids = batch['input_ids'].to(device)
                        attention_mask = batch['attention_mask'].to(device)
                        labels = batch['labels'].to(device)

                        outputs = model(input_ids, attention_mask=attention_mask)
                        predicted_scores = outputs.logits.squeeze()

                        predicted_classes = torch.round(torch.sigmoid(predicted_scores))
                        correct_predictions = (predicted_classes == labels).sum().item()
                        total_correct += correct_predictions
                        total_samples += labels.size(0)

                accuracy = total_correct / total_samples
                epoch_accuracies.append(accuracy)

                print(f"Seed {seed} - Learning Rate {learning_rate} - Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss}")

            all_epoch_losses.append(epoch_losses)
            all_final_accuracies.append(epoch_accuracies)

            # Calculate Pearson correlation coefficient
            predicted_scores_all = []
            labels_all = []

            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)

                    outputs = model(input_ids, attention_mask=attention_mask)
                    predicted_scores = outputs.logits.squeeze().cpu().numpy()

                    # Accumulate predicted scores and labels
                    predicted_scores_all.extend(predicted_scores)
                    labels_all.extend(labels.cpu().numpy())

            correlation_coefficient, _ = pearsonr(predicted_scores_all, labels_all)
            all_pearson_correlations.append(correlation_coefficient)

            # Calculate Spearman correlation coefficient
            correlation_coefficient, _ = spearmanr(predicted_scores_all, labels_all)
            all_spearman_correlations.append(correlation_coefficient)

            end_time = time.time()
            elapsed_time = end_time - start_time
            print(f"Seed {seed} - Learning Rate {learning_rate} - Time Elapsed: {elapsed_time} seconds")

            model_save_path = f"semantic_relatedness_model_lr{learning_rate}_epochs{num_epochs}_seed{seed}"
            model.save_pretrained(model_save_path)
            tokenizer.save_pretrained(model_save_path)

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Total Time taken: {elapsed_time} seconds")

# Plotting
plt.figure(figsize=(14, 10))

# Loss Analysis
plt.subplot(2, 2, 1)
for i, epoch_losses in enumerate(all_epoch_losses):
    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, label=f"Run {i+1}")
plt.xlabel('Epochs')
plt.ylabel('Training Loss')
plt.title('Training Loss vs. Epochs')
plt.legend()

# Accuracy Analysis
plt.subplot(2, 2, 2)
for i, epoch_accuracies in enumerate(all_final_accuracies):
    plt.plot(range(1, len(epoch_accuracies) + 1), epoch_accuracies, label=f"Run {i+1}")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Epochs')
plt.legend()

# Correlation Coefficient Analysis

plt.subplot(2, 2, 3)
plt.bar(range(len(all_pearson_correlations)), all_pearson_correlations, label='Pearson')
plt.xlabel('Run')
plt.ylabel('Correlation Coefficient')
plt.title('Pearson Correlation Coefficient')
plt.xticks(range(len(all_pearson_correlations)), labels=[f"Run {i+1}" for i in range(len(all_pearson_correlations))])

plt.subplot(2, 2, 4)
plt.bar(range(len(all_spearman_correlations)), all_spearman_correlations, label='Spearman')
plt.xlabel('Run')
plt.ylabel('Correlation Coefficient')
plt.title('Spearman Correlation Coefficient')
plt.xticks(range(len(all_spearman_correlations)), labels=[f"Run {i+1}" for i in range(len(all_spearman_correlations))])

plt.tight_layout()
plt.show()

# # Print out the accuracies of every run
# for i in range(len(all_final_accuracies)):
#     print(f"Run {i+1} - Final Accuracy: {all_final_accuracies[i][-1]:.4f}")

# # Detailed summary in tabular format
# summary_df = pd.DataFrame(columns=['Run', 'Learning Rate', 'Epoch', 'Average Loss', 'Accuracy'])

# for i in range(len(all_final_accuracies)):
#     for epoch, (loss, acc) in enumerate(zip(all_epoch_losses[i], all_final_accuracies[i]), 1):
#         summary_df = summary_df.append({'Run': i+1, 'Learning Rate': learning_rates[i // (len(epochs_to_run) * len(seeds))],
#                                         'Epoch': epoch, 'Average Loss': loss, 'Accuracy': acc}, ignore_index=True)

# print(summary_df)

# import matplotlib.pyplot as plt

# # Assuming you have stored the training losses from different runs in the list `training_losses`

# # Plot training loss vs. epochs for each training run
# plt.figure(figsize=(10, 6))
# for i, loss in enumerate(training_losses):
#     plt.plot(range(1, num_epochs + 1), loss, label=f"Run {i + 1}")
# plt.xlabel('Epochs')
# plt.ylabel('Training Loss')
# plt.title('Training Loss vs. Epochs')
# plt.legend()
# plt.show()


import matplotlib.pyplot as plt

# Dummy training losses for 3 runs, each with 5 epochs
training_losses = [
    [0.5, 0.4, 0.3, 0.2, 0.1],
    [0.6, 0.5, 0.4, 0.3, 0.2],
    [0.7, 0.6, 0.5, 0.4, 0.3]
]

# Plot training loss vs. epochs for each training run
plt.figure(figsize=(10, 6))
for i, loss in enumerate(training_losses):
    plt.plot(range(1, 6), loss, label=f"Run {i + 1}")
plt.xlabel('Epochs')
plt.ylabel('Training Loss')
plt.title('Training Loss vs. Epochs')
plt.legend()
plt.show()

import pandas as pd
from google.colab import drive
import os


file_path = '/content/drive/MyDrive/JMU_Wu/NLP/tel_dev_with_labels.csv'
df = pd.read_csv(file_path)


def split_sentences(sentence):
    sentences = sentence.split('\n')
    first_sentence = sentences[0].strip()
    second_sentence = sentences[1].strip() if len(sentences) > 1 else ''
    return first_sentence, second_sentence

pair_id = df['PairID']
text = df['Text']
score = df['Score']

sentences = [split_sentences(sentence) for sentence in text]

supervised_data = pd.DataFrame({
    'PairID': pair_id,
    'FirstSentence': [sentence[0] for sentence in sentences],
    'SecondSentence': [sentence[1] for sentence in sentences],
    'Score': score
})

save_path = '/content/drive/MyDrive/JMU_Wu/NLP/'
supervised_data.to_csv(os.path.join(save_path, 'tel_supervised_data.csv'), index=False)

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
import time

start_time = time.time()
end_time = 0
elapsed_time = 0

val_df = pd.read_csv('/content/drive/MyDrive/JMU_Wu/NLP/esp_supervised_data.csv')

class MultilingualDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pair = self.data.iloc[idx]
        first_sentence = pair['FirstSentence']
        second_sentence = pair['SecondSentence']

        # Tokenize sentences
        inputs = self.tokenizer(
            first_sentence,
            second_sentence,
            return_tensors='pt',
            max_length=self.max_length,
            truncation=True,
            padding='max_length'
        )

        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': torch.tensor(pair['Score'], dtype=torch.float32)
        }

model = RobertaForSequenceClassification.from_pretrained('semantic_relatedness_model')
tokenizer = RobertaTokenizer.from_pretrained('semantic_relatedness_model')

# dataset instance
val_dataset = MultilingualDataset(val_df, tokenizer)

val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model.to(device)

predicted_scores_list = []
labels_list = []

model.eval()

with torch.no_grad():
    for i, batch in enumerate(val_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        predicted_scores = outputs.logits.squeeze()

        predicted_scores_list.extend(predicted_scores.cpu().numpy())
        labels_list.extend(labels.cpu().numpy())

spearman_corr, _ = spearmanr(labels_list, predicted_scores_list)
print(f"Spearman Correlation Coefficient: {spearman_corr}")

plt.figure(figsize=(10, 6))
plt.scatter(labels_list, predicted_scores_list, color='blue', alpha=0.5)
plt.xlabel('Actual Scores')
plt.ylabel('Predicted Scores')
plt.title('Predicted Scores vs. Actual Scores')
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

data = {
    'Language': ['English', 'Telugu', 'Marathi', 'Arabic', 'Amharic', 'Hausa', 'Spanish', 'Kinyarwanda'],
    'Spearman Correlation Coefficient': [0.838, 0.51, 0.447, 0.43, 0.399, 0.42, 0.599, 0.207]
}

df = pd.DataFrame(data)

print(df)

plt.figure(figsize=(10, 6))
plt.bar(df['Language'], df['Spearman Correlation Coefficient'], color='skyblue')
plt.xlabel('Language')
plt.ylabel('Spearman Correlation Coefficient')
plt.title('Spearman Correlation Coefficients for Different Languages')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Spearman correlation coefficients obtained from my model
your_model_data = {
    'Language': ['English', 'Telugu', 'Marathi', 'Arabic', 'Amharic', 'Hausa', 'Spanish', 'Kinyarwanda'],
    'Your Model': [0.838, 0.51, 0.447, 0.43, 0.399, 0.42, 0.599, 0.207]
}

# Spearman correlation coefficients reported in the previous paper
previous_paper_data = {
    'Language': ['English', 'Telugu', 'Marathi', 'Arabic', 'Amharic', 'Hausa', 'Spanish', 'Kinyarwanda'],
    'Previous Paper': [0.60, 0.58, 0.60, 0.17, 0.57, 0.04, 0.69, 0.13]
}

df_your_model = pd.DataFrame(your_model_data)
df_previous_paper = pd.DataFrame(previous_paper_data)

merged_df = pd.merge(df_your_model, df_previous_paper, on='Language')

print("Comparison of Spearman Correlation Coefficients:")
print(merged_df)

plt.figure(figsize=(10, 6))
plt.plot(merged_df['Language'], merged_df['Your Model'], marker='o', label='Your Model', color='blue')
plt.plot(merged_df['Language'], merged_df['Previous Paper'], marker='o', label='Previous Paper', color='red')
plt.xlabel('Language')
plt.ylabel('Spearman Correlation Coefficient using XLMR')
plt.title('Comparison of Spearman Correlation Coefficients using XLMR')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()